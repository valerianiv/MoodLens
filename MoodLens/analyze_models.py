import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import f1_score

RESULTS_DIR = 'results'

model_files = {
    "SimpleCNN": "cm_simplecnn.csv",
    "FerConvNet": "cm_ferconvnet.csv",
    "EfficientNetB0_48": "cm_efficientnetb0_48.csv",
    "Random Forest": "cm_random_forest.csv",
    "SVM (RBF)": "cm_svm_(rbf).csv",
    "MLP": "cm_mlp.csv"
}

class_names = ['Anger', 'Disgust', 'Fear', 'Happiness', 'Neutral', 'Sadness', 'Surprise']
results = []

for model_name, cm_file in model_files.items():
    cm_path = os.path.join(RESULTS_DIR, cm_file)
    if not os.path.exists(cm_path):
        print(f"‚ö†Ô∏è –ü—Ä–æ–ø—É—â–µ–Ω–∞ –º–æ–¥–µ–ª—å: {cm_path}")
        continue

    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ñ–æ—Ä–º–∞—Ç –ø–æ –ø–µ—Ä–≤–æ–π —Å—Ç—Ä–æ–∫–µ —Ñ–∞–π–ª–∞
    with open(cm_path, 'r', encoding='utf-8') as f:
        first_line = f.readline().strip()

    if first_line.startswith('True_0') or first_line.startswith(',Pred_0'):
        # –§–æ—Ä–º–∞—Ç —Å –∑–∞–≥–æ–ª–æ–≤–∫–∞–º–∏ (–∫–∞–∫ —É SVM, MLP, RF —Å True_0)
        df = pd.read_csv(cm_path, index_col=0)
        cm = df.values
    else:
        # –§–æ—Ä–º–∞—Ç –±–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ (–ø–µ—Ä–≤–∞—è —Å—Ç—Ä–æ–∫–∞ = "0,1,2,...")
        df = pd.read_csv(cm_path, header=0)  # –ø–µ—Ä–≤–∞—è —Å—Ç—Ä–æ–∫–∞ ‚Äî –∑–∞–≥–æ–ª–æ–≤–∫–∏ —Å—Ç–æ–ª–±—Ü–æ–≤
        cm = df.values  # —Ç–µ–ø–µ—Ä—å 7x7

    if cm.shape != (7, 7):
        print(f"‚ùå –ú–∞—Ç—Ä–∏—Ü–∞ {model_name} –∏–º–µ–µ—Ç —Ä–∞–∑–º–µ—Ä {cm.shape}, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º.")
        continue

    # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º y_true –∏ y_pred
    y_true, y_pred = [], []
    for i in range(7):
        for j in range(7):
            count = int(cm[i, j])
            y_true.extend([i] * count)
            y_pred.extend([j] * count)

    y_true = np.array(y_true)
    y_pred = np.array(y_pred)

    macro_f1 = f1_score(y_true, y_pred, average='macro')
    accuracy = f1_score(y_true, y_pred, average='micro')
    weighted_f1 = f1_score(y_true, y_pred, average='weighted')
    f1_per_class = f1_score(y_true, y_pred, average=None)

    results.append({
        'Model': model_name,
        'Accuracy': round(accuracy, 4),
        'Macro F1': round(macro_f1, 4),
        'Weighted F1': round(weighted_f1, 4),
        **{f'F1_{class_names[i]}': round(f1_per_class[i], 4) for i in range(7)}
    })

# –í—ã–≤–æ–¥
df = pd.DataFrame(results).sort_values('Macro F1', ascending=False)
print("\nüìä –°–†–ê–í–ù–ï–ù–ò–ï –ü–û MACRO F1")
print("=" * 110)
print(df.to_string(index=False, float_format="%.4f"))

df.to_csv(os.path.join(RESULTS_DIR, 'final_comparison_by_macro_f1.csv'), index=False)

# –ì—Ä–∞—Ñ–∏–∫
plt.figure(figsize=(12, 6))
bars = plt.bar(df['Model'], df['Macro F1'], color='teal', edgecolor='black')
plt.title('–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –ø–æ Macro F1', fontsize=14)
plt.ylabel('Macro F1')
plt.xticks(rotation=45, ha='right')
for bar in bars:
    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, f'{bar.get_height():.3f}', ha='center')
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.tight_layout()
plt.savefig(os.path.join(RESULTS_DIR, 'macro_f1_comparison.png'))
plt.show()

# Heatmap –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏
best_model = df.iloc[0]['Model']
cm_path = os.path.join(RESULTS_DIR, model_files[best_model])

with open(cm_path, 'r', encoding='utf-8') as f:
    first_line = f.readline().strip()

if first_line.startswith('True_0') or first_line.startswith(',Pred_0'):
    cm_best = pd.read_csv(cm_path, index_col=0).values
else:
    cm_best = pd.read_csv(cm_path, header=0).values

plt.figure(figsize=(8, 7))
sns.heatmap(cm_best, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)
plt.title(f'Confusion Matrix ‚Äî {best_model}')
plt.xlabel('Predicted')
plt.ylabel('True')
plt.tight_layout()
plt.savefig(os.path.join(RESULTS_DIR, f'cm_best_{best_model.replace(" ", "_").lower()}.png'))
plt.show()

print(f"\nüèÜ –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å: {best_model} (Macro F1 = {df.iloc[0]['Macro F1']:.4f})")